WEBVTT

00:01.000 --> 00:07.480
In this lecture we are going to talk about the random forest classifier implementation in Python.

00:07.730 --> 00:13.240
So we are going to use this random forest classifier imported from seeking to learn.

00:13.280 --> 00:21.380
So first of all let's load the IRS data set and you can import the IDE with CXVII file but we can use

00:21.380 --> 00:24.210
the secretly data datasets instead.

00:24.230 --> 00:31.990
So from secretly our own imported data sets and this dataset contains several datasets such as Boston

00:32.000 --> 00:33.780
related house prices.

00:33.830 --> 00:38.730
We have been talking about linear regression and we have used this dataset.

00:38.870 --> 00:47.450
We can load irenÃ¦us data set digits data set divine data set breast cancer related datasets secretly

00:47.460 --> 00:50.870
and is quite convenient in this aspect as well.

00:50.930 --> 00:58.650
OK so we load the ITU is data sat again and we will have features and we will have target variables.

00:58.730 --> 01:06.620
OK so we will have 4 features as usual and the single target variable with three classes I really Seto's

01:07.070 --> 01:15.980
ideas versey color and irony's virginica we are going to use this train split in order to use 80 percent

01:15.980 --> 01:24.020
of the data set for the training and 20 percent of the original dataset is for testing purposes.

01:24.170 --> 01:31.040
So we just have to instantiate the random forest classifier with number of estimators is a thousand

01:31.340 --> 01:34.620
and the maximum features is equal still the square root.

01:34.760 --> 01:40.270
What does it mean that Hitler is going to construct a thousand decision trees.

01:40.280 --> 01:47.090
These trees are going to be under correlated because not all the features are going to be used.

01:47.090 --> 01:53.060
This is what we have been talking about in the tower articles section that ran them for us classifier

01:53.090 --> 01:58.700
searches over a random square root of and features to find the best one.

01:58.760 --> 02:02.250
As far as every split in the tree is concerned.

02:02.420 --> 02:09.200
So that's why we are going to use the square root of the features as far as constructing the decision

02:09.200 --> 02:10.520
tree is our concern.

02:10.550 --> 02:17.780
Then we are going to feed the model so we are going to train the decision tree classifier on the training

02:17.780 --> 02:25.490
data set and then we are going to make some predictions on the test dataset and we are going to calculate

02:25.690 --> 02:30.500
confusion of Matrix items and the accuracy of the algorithm.

02:30.530 --> 02:37.880
By the way you can take a look at the concrete documentation of random for us classifier so we can define

02:37.880 --> 02:42.790
lots of lots of paramita as if we do not define the number of estimators.

02:42.920 --> 02:49.160
So basically the number of decision trees being generated by a random forest classifier the default

02:49.220 --> 02:54.700
value is Tannous So it is going to use taen decision trees under the hood.

02:54.890 --> 03:02.900
We can define the criterion of whether to use the genie index approach or to use the entropy and information

03:02.930 --> 03:03.880
again approach.

03:03.980 --> 03:10.220
As far as constructing the decision trees are concerned then we have to define the maximum features.

03:10.250 --> 03:11.980
We have several options.

03:11.990 --> 03:18.530
The auto is the same as the square root where the maximum features is equal to the square root of the

03:18.530 --> 03:21.560
number of features we can use the logarithm.

03:21.650 --> 03:29.850
And if we do not define the maximum feature at all Dan the maximum features will be the number of features.

03:29.870 --> 03:37.460
So basically instead of using random for us classifier if we do not specify the maximum features we

03:37.460 --> 03:44.060
will use the standard bagging approach because this is the main difference between random for us classifier

03:44.060 --> 03:49.300
and begging that for random for us classifier on every split in the tree.

03:49.340 --> 03:54.010
A random selection of features is chosen from the full feature set.

03:54.140 --> 04:02.240
So for begging the algorithm searches over all the features and for random for us classifier the algorithm

04:02.240 --> 04:06.370
searches over a random square root and features.

04:06.440 --> 04:13.100
OK so this maximal features is the most important Peramivir for random for as classifier.

04:13.160 --> 04:15.860
So let's see whether it's working fine or not.

04:15.860 --> 04:19.240
The accuracy 0 8 is 87 percent.

04:19.370 --> 04:21.910
Maybe we should use cross-validation instead.

04:22.010 --> 04:26.760
If you read on the algorithm it's going to be 98 percent.

04:26.870 --> 04:30.160
If you run again 98 percent.

04:30.230 --> 04:32.600
I have managed to end up with one.

04:32.600 --> 04:35.320
So 100 percent accuracy.

04:35.570 --> 04:38.950
OK it is approximately 97 percent accurate.

04:38.960 --> 04:45.710
So the random forest classifier is approximately 97 percent accurate in the coming lectures.

04:45.830 --> 04:51.320
We are going to talk about how to tune around for classifier even more.

04:51.320 --> 04:52.190
Thanks for watching.
