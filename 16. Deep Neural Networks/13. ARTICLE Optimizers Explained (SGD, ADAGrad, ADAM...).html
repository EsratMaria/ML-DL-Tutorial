<p>If you are interested in the optimizers/updaters (such as Gradient Descent, Momentum, ADAGrad, ADAM ...)&nbsp;check out this article on this subject:&nbsp;</p><p>http://www.globalsoftwaresupport.com/optimization-algorithms-deep-learning/<br></p>