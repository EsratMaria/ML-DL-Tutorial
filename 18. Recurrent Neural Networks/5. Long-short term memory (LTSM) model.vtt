WEBVTT

00:00.890 --> 00:04.820
So far we have been talking about recurrent and our on that works.

00:04.820 --> 00:12.140
We have been talking about wenching gradient problem and we have come to the conclusion that using LSD

00:12.490 --> 00:18.820
or long short term memory architecture can solve the vanishing gradient problem.

00:18.860 --> 00:20.640
So instead of the age.

00:20.640 --> 00:30.050
Units so age the age D-plus so on age D-plus too we want to order some memory in our own network and

00:30.140 --> 00:32.880
we want to manipulate these memory cells.

00:32.990 --> 00:35.840
First of all we want to flush the memory.

00:35.960 --> 00:39.260
So we would like to get rid of some information.

00:39.320 --> 00:42.880
So that's why we have to create the forget gate.

00:43.020 --> 00:47.500
Of course sometimes we want to order some new information to the memory.

00:47.600 --> 00:50.570
So this is why we have to design the input gate.

00:50.780 --> 00:54.360
And sometimes you want to read some information from the memory.

00:54.440 --> 00:57.550
And this is why we have to create the out the gate.

00:57.680 --> 01:05.060
So instead of just a single hidden layer by the way it is important to know that this didn't know contains

01:05.090 --> 01:06.600
several name rowans.

01:06.620 --> 01:13.040
So this is what we have been discussing in the first lecture of distracter that this is how we represent

01:13.040 --> 01:19.820
recurrent in our own networks that every single node represents a given layer with several and there

01:19.820 --> 01:20.440
were loans.

01:20.540 --> 01:27.620
So this is just a node in the recurrent now our network but it contains stree now rowans as the hidden

01:27.620 --> 01:28.360
layer.

01:28.460 --> 01:33.990
OK so these nodes age the age D-plus want at age D-plus 2.

01:34.020 --> 01:35.540
These are layers.

01:35.540 --> 01:38.900
Basically the hidden layers with several now with bones.

01:38.990 --> 01:46.820
But the main feature of long short term memory architecture is to use gates forget gate impled gate

01:46.910 --> 01:48.020
out to the gate.

01:48.020 --> 01:51.380
So we are going to deal with memory as well.

01:51.440 --> 01:58.610
OK so there are the age parameters and we will have the and pairing meters which are the memory cells

01:58.670 --> 01:59.850
basically.

01:59.900 --> 02:00.600
OK.

02:00.650 --> 02:08.840
So as far as the output is concerned this gate determines the age outboard based on the previous taps

02:08.900 --> 02:10.440
stored in the memory.

02:10.460 --> 02:18.230
So we are going to store lots of lots of information in the memory and the output gate is going to determine

02:18.410 --> 02:23.210
that what data stored in the memory is important or not.

02:23.210 --> 02:30.110
OK so what's the main concept behind long short term memory that if you take a look at a given image

02:30.410 --> 02:37.520
during the training procedure there are going to be lots of lots of iterations for example in the first

02:37.580 --> 02:44.510
iteration the algorithm comes to the conclusion that the most relevant feature of a cat is the shape

02:44.510 --> 02:45.670
of the ears.

02:45.740 --> 02:51.100
Then in the next iterations there are going to detect several other features.

02:51.260 --> 02:58.850
And the algorithm midsouth forgets about the first feature as far as the shape of the years are concerned.

02:58.850 --> 03:06.080
So that's why we would like to store the shape of the ears as a relevant feature in the memory in order

03:06.080 --> 03:12.410
to make sure in further iterations that we remember these important features.

03:12.560 --> 03:21.350
So that's why we have to store information in memory cells and the algorithm midsouth is going to remember

03:21.530 --> 03:25.430
the most relevant features the most relevant information.

03:25.610 --> 03:33.280
OK so the output gate is going to determine the output based on the previous data stored in memory.

03:33.350 --> 03:36.590
OK so the equation is quite straightforward.

03:36.590 --> 03:40.680
First of all it's important to use the sigmoid activation function.

03:40.850 --> 03:45.440
So we have the age t minus one and the X the input.

03:45.440 --> 03:50.110
Of course we will use some weight parameters double 0 0.

03:50.190 --> 03:55.640
OK we have to use the sam as usual and then we have to use an activation function.

03:55.670 --> 04:02.120
As usual the activation function is going to be the sigmoid activation function because we are after

04:02.180 --> 04:05.370
a number within the range of 0 and 1.

04:05.510 --> 04:12.980
So we can play the values in the memory with the half of this output Gade the value after applying the

04:12.980 --> 04:15.500
sigmoid function here is zero.

04:15.500 --> 04:22.400
What does it mean that we do not care about the information present in the memory zero times anything

04:22.400 --> 04:24.940
in the memory is going to be zero.

04:24.980 --> 04:29.410
It's not going to have any effect on the training procedure.

04:29.600 --> 04:38.060
On the other hand if the value after applying the activation function is 1 It means that we are curious

04:38.150 --> 04:42.000
about the information present in the memory cell.

04:42.200 --> 04:50.330
OK so this is how we calculate HD it's going to be the sigmoid activation function of x d and h t minus

04:50.330 --> 04:57.200
1 we given educates and of course we have to use the bias and we have to deal with information present

04:57.290 --> 04:58.870
in memory south.

04:58.950 --> 05:02.980
OK these are ration is an element wise multiplication.

05:03.140 --> 05:09.710
And this is how we make sure that we store lots of lots of information in the memory such as the shape

05:09.710 --> 05:16.170
of the ears such as the coloring the background or for example the color of eyes and so on.

05:16.220 --> 05:24.170
And during the training procedure recurrent now our networks and our t s is going to come to the conclusion

05:24.170 --> 05:31.770
that the color of the background is not that important the shape of the ears is very very important.

05:31.850 --> 05:39.530
So the output gate is going to have the value zero as far as the background color is concerned because

05:39.530 --> 05:47.300
this feature this information is not relevant when we are dealing with the shape of the ears then the

05:47.300 --> 05:54.470
value of the output will be won because we would like to use this relevant feature present in the memory.

05:54.560 --> 05:58.580
So this is how we select what's important and what's not.

05:58.670 --> 06:06.160
And during the training procedure we update this double zero educates in order to make Digby's prediction.

06:06.320 --> 06:14.640
OK so that's all about the output Gade the output gate is going to control the HD output of course.

06:14.660 --> 06:17.820
Somehow we have too many plate memory cells.

06:18.000 --> 06:21.510
What to keep in the memory and what to discard.

06:21.590 --> 06:29.000
And basically this is the aim of forgat gate with the help of the forget gate we manipulate not the

06:29.000 --> 06:32.050
output but the content of the memory.

06:32.110 --> 06:34.060
We want to store in the memory.

06:34.100 --> 06:41.750
So what are the relevant information and what's not even the output of the forget gate is 0 what does

06:41.750 --> 06:42.470
it mean.

06:42.470 --> 06:49.370
It means that we get rid of the information present in the actual memory south because it is not important

06:49.610 --> 06:56.780
if the output of the forget gate is one we keep all the information present in the memory because it

06:56.780 --> 07:00.550
is important in our classification for example.

07:00.680 --> 07:07.100
And that's why it's crucial to use the sigmoid activation function to end up within the range 0 and

07:07.100 --> 07:07.660
1.

07:07.760 --> 07:14.210
If we use 10 age activation function it's going to be within the range of minus 1 and plus 1.

07:14.300 --> 07:15.790
So it's not going to work.

07:15.800 --> 07:20.770
We have to use sigmoid activation functions for Gates anyways.

07:20.960 --> 07:25.530
As you can see the edge weights are different for the forget gate.

07:25.580 --> 07:27.430
As for the output gate.

07:27.560 --> 07:34.750
So during the training procedure the algorithm is going to find the optimal weights for the output gate

07:34.850 --> 07:36.530
for the forgat gate.

07:36.590 --> 07:38.710
And of course for the input gate.

07:38.810 --> 07:43.730
So what about the input gate the input is going to be something like this.

07:43.730 --> 07:47.080
We have H D minus 1 because it is a recurrence.

07:47.090 --> 07:51.710
Now we're all in that work so we have to deal with that information in the previous stop.

07:51.770 --> 07:55.710
We have to deal with the information in the actual step x t.

07:55.820 --> 07:59.210
Of course there's going to be some way to Matrix.

07:59.300 --> 08:04.540
We have to sum them up and we have to use the 10 H activation function.

08:04.670 --> 08:09.550
In this case you can use sigmoid you can use the ralliers activation function.

08:09.560 --> 08:17.050
Usually we use the tan age and with the half of the input Kate with sigmoid activation function again

08:17.330 --> 08:25.190
in order to transform the value within the range 0 and 1 we can control what to keep from the input

08:25.370 --> 08:29.900
because there are lots of lots of irrelevant information in the input.

08:29.990 --> 08:36.170
I don't want to repeat myself over and over again but if we had an image for example there are lots

08:36.170 --> 08:42.830
of lots of irrelevant information what's the color of the background or what's the size of the image

08:43.100 --> 08:46.120
or where is the object located on the given image.

08:46.190 --> 08:52.940
So there are lots of lots of important information present within the image and lots of lots of irrelevant

08:52.970 --> 08:54.100
information.

08:54.230 --> 09:01.310
OK so with the improved gait we kind of write new data to the memory and we read another a gateway into

09:01.310 --> 09:01.590
the.

09:01.610 --> 09:06.830
Basically we can control what data to keep and what to discard.

09:06.920 --> 09:09.140
So just to summarize it again.

09:09.240 --> 09:13.290
LS LSD is going to deal with memory cells.

09:13.370 --> 09:20.120
So in the hidden layer we are going to have the activations but we are going to have the memory cell

09:20.120 --> 09:21.520
values as well.

09:21.560 --> 09:29.090
We have the age of values and we have the Capitol and the values representing the memory and with the

09:29.090 --> 09:37.280
help of street Gates forget gate into the gate and out to the gate we can manually the output and we

09:37.280 --> 09:43.940
can manipulate the memory cells whether to flush the memory whether to order a given item to the memory

09:44.090 --> 09:46.070
or read from the memory.

09:46.160 --> 09:53.780
OK so we the hack of Gates we are able to make sure that we keep the most relevant features and we are

09:53.780 --> 09:57.640
able to deal with the data far away from each other.

09:57.650 --> 10:04.840
So with the half of the memory make sure that we are able to detect long term dependencies.

10:04.910 --> 10:11.320
So for example sentences in a given tax far away from each other are well trained to be current.

10:11.310 --> 10:12.560
Now we're on a network.

10:12.610 --> 10:21.200
Did at the as an architecture is able to come to the conclusion that OK this guy speaks fluent Hungarian

10:21.550 --> 10:23.670
because he is from Hungary.

10:23.750 --> 10:30.290
So we are able to deal with items quite far away from each other in the training procedure.

10:30.380 --> 10:32.610
And this is exactly what we are after.

10:32.660 --> 10:39.120
Ok the only problem with these long short term memory is that it is quite a complex model.

10:39.140 --> 10:41.280
We have to deal with the output again.

10:41.360 --> 10:44.000
We have to deal with the forgat gate.

10:44.030 --> 10:46.200
We have to deal with the input again.

10:46.310 --> 10:50.360
There are lots of lots of ways to the day to double you zero.

10:50.360 --> 10:53.520
Double you want w to use W3.

10:53.540 --> 10:57.580
So the training procedure is going to be extremely slow.

10:57.740 --> 11:04.250
And this is why gated recurrent units came to be and this is what we are going to talk about in the

11:04.250 --> 11:05.490
coming lecture.

11:05.630 --> 11:06.470
Thanks for watching.
